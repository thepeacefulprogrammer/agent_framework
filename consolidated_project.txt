# Consolidated Project Files
# Generated on: 2025-08-09 15:41:14
# Source directory: /home/randy/workspace/personal/agent_framework
# Total files: 9

# File Summary:
#   Python: 9 files
# ==============================================================================

# TABLE OF CONTENTS
# ------------------------------------------------------------------------------

# Python Files:
#   1. learn_api.py
#   2. run_graph.py
#   3. src/minimal_agent_framework/__init__.py
#   4. src/minimal_agent_framework/graph.py
#   5. src/minimal_agent_framework/node.py
#   6. src/minimal_agent_framework/tool.py
#   7. src/minimal_agent_framework/utils.py
#   8. tests/__init__.py
#   9. tests/test_graph.py

# ==============================================================================


################################################################################
# PYTHON FILES
################################################################################

================================================================================
# File: learn_api.py
# Type: Python
================================================================================


from dotenv import load_dotenv
from minimal_agent_framework import tool, call_llm, EventEmitter
from pydantic import BaseModel

import logging
logging.basicConfig(level=logging.INFO)

for name in ("httpx", "httpcore"):
    lg = logging.getLogger(name)
    lg.setLevel(logging.ERROR)

load_dotenv()

@tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers together."""
    return a + b

class OutputTest(BaseModel):
    """Test output model."""
    output_process_used: str
    output_text: str

def handler(x: str):
    print(f"{x}", end='', flush=True)


if __name__ == "__main__":

    events = EventEmitter()
    events.on("text", handler)

    response = call_llm(input="use the add_numbers tool to add 5 and 3, and then use it again to add 10 and 20. Respond with the output text that will be seen by the user, and also return the process used to generate the output.", events=events)    


================================================================================
# File: run_graph.py
# Type: Python
================================================================================

from src.minimal_agent_framework import Graph, Node, EventEmitter
import logging
from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO)

for name in ("httpx", "httpcore"):
    lg = logging.getLogger(name)
    lg.setLevel(logging.ERROR)

load_dotenv()

def sample_pre_function():
    logging.debug("Pre-function executed")

def sample_post_function():
    logging.debug("Post-function executed")

def handler(x: str):
    print(f"{x}", end='', flush=True)

if __name__ == "__main__":
    # Example usage of Graph and Node
    
    events = EventEmitter()
    events.on("text", handler)

    graph = Graph(events)
    
    node1 = (Node()
             .name("first")
             .input("Hi there!")
             .instructions("Speak like a prirate")
             .routes([{"second": "default"}])
             .pre(sample_pre_function)
             .post(sample_post_function))

    node2 = (Node()
             .name("second")
             .input("Do you have a name?")
             .routes([{"third": "default"}])
             .pre(sample_pre_function)
             .post(sample_post_function))

    node3 = (Node()
             .name("third")
             .input("Do you know what today's date is?")
             .post(sample_post_function)
             )
    
    graph.add_nodes([node1, node2, node3])
    graph.run(node1)

================================================================================
# File: src/minimal_agent_framework/__init__.py
# Type: Python
================================================================================

from .graph import Graph
from .node import Node
from .tool import tool, ToolRegistry
from .utils import call_llm, EventEmitter

__all__ = ['Graph', 'Node', 'tool', 'ToolRegistry', 'call_llm', 'EventEmitter']

================================================================================
# File: src/minimal_agent_framework/graph.py
# Type: Python
================================================================================

from .node import Node
from .utils import EventEmitter
import logging

class Graph():
    def __init__(self, events : EventEmitter | None = None):
        self.nodes: list[Node] = []
        self.events = events
    
    def add(self, node: Node) -> 'Graph':
        if not isinstance(node, Node):
            raise TypeError("Node must be an instance of Node.")
        self.nodes.append(node)
        return self

    def add_nodes(self, nodes: list[Node]) -> 'Graph':
        for node in nodes:
            if not isinstance(node, Node):
                raise TypeError("All items must be instances of Node.")
            self.nodes.append(node)
        return self
    
    def run(self, starting_node: Node):
        if not self.nodes:
            raise RuntimeError("Graph has no nodes to run.")
        if starting_node not in self.nodes:
            raise ValueError("Starting node must be part of the graph's nodes.")
        node_lookup = {node._name: node for node in self.nodes}
        next_node = starting_node.execute(self.events)
        while len(next_node) > 0:
            if next_node not in node_lookup:
                raise ValueError(f"Node {next_node} not found in graph nodes.")
            for node in self.nodes:
                if node._name == next_node:
                    next_node = node.execute(self.events)
                    break
        else:
            print("No next node to run.")

================================================================================
# File: src/minimal_agent_framework/node.py
# Type: Python
================================================================================

import logging
from typing import Optional, Callable
from .utils import EventEmitter, call_llm

class Node():
    def __init__(self):
        self._name = ""
        self._routes: list[dict[str, str]] = []
        self._pre_func: Optional[Callable] = None
        self._post_func: Optional[Callable] = None
        self._instructions: Optional[str] = None
        self._input: Optional[str] = None

    def name(self, name: str) -> 'Node':
        self._name: str = name
        return self

    def input(self, input: str) -> 'Node':
        self._input = input
        return self

    def instructions(self, instructions: str) -> 'Node':
        self._instructions = instructions
        return self

    def routes(self, routes: list[dict[str, str]]) -> 'Node':
        self._routes = routes
        return self
    
    def pre(self, func: Callable) -> 'Node':
        self._pre_func = func
        return self
    
    def post(self, func: Callable) -> 'Node':
        self._post_func = func
        return self
    
    def execute(self, events : EventEmitter | None = None) -> str:
        logging.debug(f"Executing node: {self._name}")
        
        if self._pre_func:
            logging.debug(f"Running pre-function for node: {self._name}")
            self._pre_func()

        if events:
            if self._input and self._instructions:
                call_llm(self._input, instructions=self._instructions, events=events)
            elif self._input:
                call_llm(self._input, events=events)

        if self._post_func:
            logging.debug(f"Running post-function for node: {self._name}")
            self._post_func()

        if self._routes:
            if len(self._routes) == 1:
                return list(self._routes[0].keys())[0]
        return ""

================================================================================
# File: src/minimal_agent_framework/tool.py
# Type: Python
================================================================================

import inspect
import json
import logging
from typing import Any, Callable, Dict, List, Optional, get_type_hints

import openai
from pydantic import BaseModel, create_model

class ToolRegistry:
    """Function-only tool registry for OpenAI Responses API."""
    _funcs: Dict[str, Callable[..., Any]] = {}
    _schemas: Dict[str, Any] = {}
    _order: List[str] = []  # preserves registration order

    @classmethod
    def register(
        cls,
        func: Callable[..., Any],
        *,
        name: Optional[str] = None,
        description: Optional[str] = None,
        replace: bool = False,
    ) -> Callable[..., Any]:
        tool_name = name or func.__name__
        if not replace and tool_name in cls._funcs:
            raise ValueError(f"Tool '{tool_name}' is already registered.")

        model = _build_model_from_func(func, tool_name, description)
        tool_schema = openai.pydantic_function_tool(model)

        cls._funcs[tool_name] = func
        cls._schemas[tool_name] = tool_schema
        if tool_name not in cls._order:
            cls._order.append(tool_name)

        logging.debug(f"Registered tool: {tool_name}")
        return func  # keep original callable behavior

    @classmethod
    def get_tools(cls) -> List[Any]:
        """Return tool schemas for Responses API."""
        return [cls._schemas[name] for name in cls._order]

    @classmethod
    def call(cls, name: str, args: Any) -> Any:
        """Invoke a registered tool by name with dict or JSON-encoded args."""
        if name not in cls._funcs:
            raise KeyError(f"Tool '{name}' not found. Available: {list(cls._funcs.keys())}")

        fn = cls._funcs[name]

        if isinstance(args, str):
            try:
                args = json.loads(args) if args else {}
            except json.JSONDecodeError:
                logging.warning(f"Arguments for tool '{name}' not valid JSON; passing raw string")
        if args is None:
            args = {}
        if not isinstance(args, dict):
            raise TypeError(f"Tool '{name}' expects dict args, got {type(args).__name__}")

        return fn(**args)

    @classmethod
    def reset(cls):
        """Clear all registered tools (useful in tests)."""
        cls._funcs.clear()
        cls._schemas.clear()
        cls._order.clear()


def tool(
    func: Optional[Callable[..., Any]] = None,
    *,
    name: Optional[str] = None,
    description: Optional[str] = None,
    replace: bool = False
):
    """
    Decorator to turn a function into a registered tool.

    @tool
    def my_tool(a: int) -> str: ...

    @tool(name="custom", description="...", replace=True)
    def other(...): ...
    """
    def _decorator(f: Callable[..., Any]):
        return ToolRegistry.register(f, name=name, description=description, replace=replace)

    return _decorator if func is None else _decorator(func)


def _build_model_from_func(func: Callable[..., Any], model_name: str, description: Optional[str]) -> type[BaseModel]:
    """
    Build a Pydantic model from the function signature.
    - Types come from annotations (supports Annotated[...] with Field(...)).
    - Defaults come from function defaults.
    - Zero-arg functions are supported.
    """
    sig = inspect.signature(func)
    # include_extras=True preserves Annotated metadata (e.g., Field)
    hints = get_type_hints(func, include_extras=True)  # type: ignore[arg-type]

    # Dict[str, Any] helps Pylance not over-constrain kwargs to create_model
    fields: Dict[str, Any] = {}
    for param in sig.parameters.values():
        if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
            raise TypeError(f"@tool does not support *args/**kwargs for '{model_name}'.")

        ann = hints.get(param.name, Any)
        default = param.default if param.default is not inspect._empty else ...
        fields[param.name] = (ann, default)

    # Avoid passing __doc__ to create_model; set it after. __module__ is safe.
    model = create_model(
        model_name,
        __module__=func.__module__,
        **fields,
    )
    model.__doc__ = description or (func.__doc__ or "")

    return model

================================================================================
# File: src/minimal_agent_framework/utils.py
# Type: Python
================================================================================

from openai import OpenAI
from pydantic import BaseModel
import os
from openai import OpenAI
from openai.types.responses import ResponseFunctionToolCall, ParsedResponse
import json
from pydantic import BaseModel
import logging
from minimal_agent_framework.tool import ToolRegistry
from typing import Callable

class EventEmitter:
    def __init__(self):
        self._listeners = {}

    def on(self, event: str, callback: Callable):
        if event not in self._listeners:
            self._listeners[event] = []
        self._listeners[event].append(callback)

    def emit(self, event: str, *args, **kwargs):
        if event in self._listeners:
            for callback in self._listeners[event]:
                callback(*args, **kwargs)

def call_llm(input: str | list, instructions: str | None = None, response_id: str | None = None, output: type[BaseModel] | None = None, events: EventEmitter | None = None) -> ParsedResponse:
    """Call the OpenAI LLM with the provided input and return the response."""

    api_key = os.getenv("AZURE_API_KEY")
    base_url = os.getenv("AZURE_API_ENDPOINT")

    client = OpenAI(api_key=api_key, base_url=base_url, default_query={"api-version": "preview"})

    if isinstance(input, str):
        input = [{
            "role": "user", 
            "content": input,
        }]

    kwargs = {}
    if response_id:
        kwargs['previous_response_id'] = response_id
    if output:
        kwargs['text_format'] = output
    if instructions:
        kwargs['instructions'] = instructions

    logging.debug(f"Calling LLM with input: {input} and response_id: {response_id}")

    with client.responses.stream(
        input=input,
        model="o4-mini",
        tools=ToolRegistry.get_tools(),
        **kwargs
        ) as stream:
            tool_calls = []
            previous_response_id = ""

            for event in stream:
                if event.type == "response.created":
                    previous_response_id = event.response.id
                elif event.type == "response.output_text.delta" and events:
                    events.emit("text", event.delta)
                elif event.type == "response.output_item.done":
                    tool_call = event.item
                    if isinstance(tool_call, ResponseFunctionToolCall):
                        tool_output = ToolRegistry.call(tool_call.name, tool_call.arguments)
                        tool_calls.append({
                            "type": "function_call_output",
                            "call_id": tool_call.call_id,
                            "output": json.dumps(tool_output) if isinstance(tool_output, dict) else str(tool_output)

                        })
            if len(tool_calls) > 0:
                return call_llm(tool_calls, previous_response_id, output=output, events=events)
        
    return stream.get_final_response()

================================================================================
# File: tests/test_graph.py
# Type: Python
================================================================================

from unittest.mock import MagicMock
from minimal_agent_framework import Graph, Node

def test_graph_creation():
    graph = Graph()
    assert graph is not None
    assert isinstance(graph, Graph)

def test_graph_has_nodes_list():
    graph = Graph()
    assert hasattr(graph, 'nodes')
    assert isinstance(graph.nodes, list)
    assert len(graph.nodes) == 0

def test_node_creation():
    node = Node()
    assert node is not None
    assert isinstance(node, Node)

def test_add_node_to_graph():
    graph = Graph()
    node = Node()
    graph.nodes.append(node)
    assert len(graph.nodes) == 1
    assert graph.nodes[0] is node
    assert isinstance(graph.nodes[0], Node)

def test_graph_has_starting_node_attribute():
    graph = Graph()
    assert hasattr(graph, 'starting_node')
    assert graph.starting_node is None
    
def test_starting_node_starts_with_graph_run():
    graph = Graph()
    mock_node = MagicMock()
    graph.nodes.append(mock_node)
    graph.run(mock_node)
    mock_node.execute.assert_called_once()

def test_starting_node_fails_if_node_has_not_been_added():
    graph = Graph()
    mock_node = MagicMock()
    other_node = MagicMock()
    graph.nodes.append(other_node)
    try:
        graph.run(mock_node)
        assert False, "Expected ValueError when starting node is not in graph"
    except ValueError as e:
        assert str(e) == "Starting node must be part of the graph's nodes."

def test_running_node_returns_next_node_name_to_run():
    graph = Graph()
    node_1 = Node().name("node_1").routes([{"node_2": "default"}])
    node_2 = Node().name("node_2")
    graph.nodes.append(node_1)
    graph.nodes.append(node_2)
    graph.run(node_1)
    assert node_1.execute() == "node_2"

def test_graph_executes_next_node():
    graph = Graph()
    mock_node_1 = MagicMock()
    mock_node_2 = MagicMock()
    mock_node_1._name = "node_1"
    mock_node_2._name = "node_2"
    mock_node_1.execute.return_value = "node_2"
    mock_node_2.execute.return_value = ""

    graph.nodes.append(mock_node_1)
    graph.nodes.append(mock_node_2)
    graph.run(mock_node_1)
    mock_node_1.execute.assert_called_once()
    mock_node_2.execute.assert_called_once()

def test_node_executes_pre_and_post_functions():

    mock_pre = MagicMock()
    mock_post = MagicMock()

    graph = Graph()
    node_1 = Node().name("node_1").routes([{"node_2": "default"}]).pre(mock_pre).post(mock_post)
    node_2 = Node().name("node_2")


    graph.nodes.append(node_1)
    graph.nodes.append(node_2)
    graph.run(node_1)

    mock_pre.assert_called_once()
    mock_post.assert_called_once()

